{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Strategies\n",
    "We can do a value interation (inferior) or policy interation (superior) strategy to find the best action or policy to take."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Value interation to find the optimal policy is very expensive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2.   -0.   -0.   -0.]\n",
      " [  -0.   nan   -0. -100.]\n",
      " [  -0.   -0.   -0.  100.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "blocked = [(1,1)]\n",
    "goal = (2,3)\n",
    "death = (1,3)\n",
    "def build_env():\n",
    "    arr = np.negative(np.zeros([3,4]))\n",
    "    arr[goal] = 100\n",
    "    arr[death] = -100\n",
    "    for wall in blocked:\n",
    "        arr[wall] = None\n",
    "    arr[0,0] = 2\n",
    "    return arr\n",
    "env = build_env()\n",
    "print (env)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## what is the value function?\n",
    "two types:\n",
    "let $ E $ = expected\n",
    "let $ \\pi $ = policy ( the action )\n",
    "let $ * $ = optimal ( best )\n",
    "- State-Value Function ( $ V^{*}(s) = R(s) + \\gamma{\\max_a}Q^{*}(s,a) $ )\n",
    "    - Value of the state\n",
    "    - This is the total reward of the system( final value)\n",
    "- Action-Value Function ( $ Q^{*}(s,a) = \\Sigma_{s'}{P(s' \\mid s,a)V^{\\ast}(s') $ )\n",
    "    - Value of the action\n",
    "    - Per state\n",
    "the reward function is the same:   $ {G}_t = \\sum_{k=0}^{\\infty} {\\gamma ^ k}{R}_{t+1+k} $\n",
    "\n",
    "bellman's equation is plug in the best action $ Q^* $ solving for the best  $ V^{\\ast} = R(s) + \\gamma{\\max_a} \\Sigma_{s'}{P(s' \\mid s,a)V^{\\ast}(s') $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Value interation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "(1, 3)\n",
      "(2, 2)\n",
      "[[   2.   -0.   -0.   -0.]\n",
      " [  -0.   nan   -0. -100.]\n",
      " [  -0.   -0.   -0.  100.]]\n",
      "[(2, 2)]\n"
     ]
    }
   ],
   "source": [
    "location = (2, 3)\n",
    "directions = {(0, 1) : [None, None],\n",
    "              (1, 0) : [None, None],\n",
    "              (-1, 0) : [None, None],\n",
    "              (0, -1) : [None, None]}\n",
    "val_map = []\n",
    "gamma = 1\n",
    "reward = -4\n",
    "def allowed_states(state):  # get adjacent spaces\n",
    "    p = []\n",
    "    for direction in directions.keys():\n",
    "        r, c = tuple([sum(x) for x in zip(direction, state)])\n",
    "        r = min(r,2) # hit wall\n",
    "        c = min(c,3)\n",
    "        if (r, c) in blocked or (r, c) == state: # hit any obstacles\n",
    "            r, c = state  # np.multiply((-1, -1), state)\n",
    "        coordinate = (r, c)\n",
    "        print (coordinate)\n",
    "        directions[direction] = coordinate\n",
    "        if coordinate == death or coordinate == state or coordinate == goal:\n",
    "            continue\n",
    "        else:\n",
    "            p.append((r,c))\n",
    "    return p\n",
    "\n",
    "\n",
    "prev = allowed_states(location)\n",
    "print (env)\n",
    "print (prev)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only one previous state allows us to get here:\n",
    "we need to calculate the bellman's equation for that state: $ V^{\\ast} = R(s) + \\gamma{\\max_a} \\Sigma_{s'}{P(s' \\mid s,a)V^{\\ast}(s') \\quad for~R(s)=0.04 \\quad \\gamma=1$\n",
    "for previous states: calculate each action:\n",
    "- Up: $ 0.4 + (1) * (  ~  (V_{12}*0.8) + (V_{21}*0.1) + (V_{23}*0.1)    ~   ) \\quad \\forall V = 0, ~ V_{23}=100   $\n",
    "- Down: $ 0.4 + (1) * (  ~  (V_{22}*0.8) + (V_{11}*0.1) + (V_{23}*0.1)    ~   ) \\quad \\forall V = 0, ~ V_{23}=100   $\n",
    "- Left $ 0.4 + (1) * (  ~  (V_{11}*0.8) + (V_{12}*0.1) + (V_{22}*0.1)    ~   ) \\quad \\forall V = 0, ~ V_{23}=100   $\n",
    "- Right: $ 0.4 + (1) * (  ~  (V_{23}*0.8) + (V_{22}*0.1) + (V_{12}*0.1)    ~   ) \\quad \\forall V = 0, ~ V_{23}=100   $\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 2)\n",
      "(1, 2)\n",
      "(2, 1)\n",
      "{(2, 2): 76.0}\n"
     ]
    }
   ],
   "source": [
    "def bellman(vp, vs, vss):\n",
    "    vpi = gamma*(  (vp*0.8) +  (vs*0.1)  + (vss*0.1)  )\n",
    "    return vpi\n",
    "\n",
    "def calculate(states):\n",
    "    env_val = {}\n",
    "    for state in states:\n",
    "        expected_reward=0\n",
    "        allowed_states(state)\n",
    "        for direction in directions:  #  find the two adjacent sides geographic location\n",
    "            vp = directions[direction]\n",
    "            if direction[0] != 0:  # Vertical movement\n",
    "                vs = directions [(0, 1)]\n",
    "                vss = directions [(0, -1)]\n",
    "            else:  # lateral movement\n",
    "                 vs = directions [(1, 0)]\n",
    "                 vss = directions [(-1, 0)]\n",
    "\n",
    "            vp = env[vp]  # expected_reward\n",
    "            vs = env[vs]\n",
    "            vss = env[vss]\n",
    "\n",
    "            expected_reward = max(bellman(vp,vs,vss), expected_reward) # math\n",
    "        env_val[state] = reward + expected_reward\n",
    "    return env_val\n",
    "\n",
    "g = calculate(prev)\n",
    "print(g)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}