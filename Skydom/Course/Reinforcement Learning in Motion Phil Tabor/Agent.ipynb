{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pseudo code algorithm\n",
    "- Initialize G randomly\n",
    "- Repeat epoch\n",
    "   - while game\n",
    "     - Get state / reward from env\n",
    "     - select action\n",
    "     - get updated state / reward\n",
    "      - store new state / reward\n",
    "- Replay memory of prev. epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different serialization formats:\n",
    "- HDF5\n",
    "- Pickle\n",
    "- Apache:\n",
    "  - Feather (arrow)\n",
    "  - Parquet\n",
    "  \n",
    "The file formats are depend on speed and compatibility. I would rather speed at this point. I need to optimize for a single system (linux) - Pickle would be the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "import jdc  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='mylog.log',\n",
    "    level=logging.DEBUG,\n",
    "    format=' %(asctime)s - %(levelname)s - %(message)s',\n",
    "\n",
    "    )\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "interact with environment \n",
    "does not choose reward\n",
    "has to have: \n",
    "- learning \n",
    "- choose action\n",
    "- updating memory \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set learning rate alpha \\\n",
    "G is the Q table \\\n",
    "we have to initialize the q table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction ={\n",
    "    'U' : [1, -1, 0],\n",
    "    'R': [2, 0, 1],\n",
    "    'D': [3 , 1 , 0],\n",
    "    'L' : [4 , 0 , -1]\n",
    "    }\n",
    "class Agent (object):\n",
    "    def __init__(self, states, alpha=0.15, randomFactor=0.2) -> None:\n",
    "        self.state_history = None\n",
    "        self.alpha = 0.1\n",
    "        self.G = {}\n",
    "        self.initReward(states)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">  The equation used to update the rewards is a derivative of the SMA (bellman's equation):\n",
    "$$ SMA = {\\Sigma _ {n}  a _ {1} + a _ {2} + \\ldots  a _ {n} \\over n} = {{\\hat {x}_{n-1}} + { 1\\over n}( x_n-\\hat {x}_{n-1} )} $$\n",
    "$$ G_{state} = G_{state} + \\alpha (\\mathfrak{target}-G_{state}) $$\n",
    "\n",
    "if alpha is: \n",
    "- 1 = target\n",
    "$$ G_{state} = G_{state} + \\mathfrak{target} -G_{state} = \\mathfrak{target} $$\n",
    "\n",
    "- 0 = current\n",
    "$$ G_{state} = G_{state} $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def learning (self) -> None:\n",
    "    target = 0 # after epoche\n",
    "    for prev, reward in reversed(self.stateHistory):\n",
    "        self.G[prev] = self.G + self.alpha* (target - self.G[prev])\n",
    "        target += reward\n",
    "        self.stateHistory[None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose_action\n",
    "- action space\n",
    "  - dictionary of actions\n",
    "- choose action\n",
    "  - look at G state - reward\n",
    "- update state history\n",
    "  - addstate to history\n",
    "* forgot to do exploration v exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def choose_action(self, states, allowedMoves) -> int:\n",
    "\n",
    "        reward = 0\n",
    "        nextState = (None, None)\n",
    "        robot = np.nonzero(env == 2) #get robot location\n",
    "        randomN = np.random.random()\n",
    "        if randomN > self.randomFactor:\n",
    "                for a, x, y in allowedMoves.values(): #pick best action | reward\n",
    "                        nextState[0] = robot[0]+x \n",
    "                        nextState[1] = robot[1]+y\n",
    "                        if reward < self.G[nextState]: # get reward\n",
    "                                choice = nextState\n",
    "        else: \n",
    "                nextMove = np.random.choice(allowedMoves)\n",
    "\n",
    "\n",
    "        self.stateHistory.append (choice) # add to state history\n",
    "        return choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mem_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def mem_update (self) -> None:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q table is initialized here with a random distribution between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Agent\n",
    "def initReward (self, states) -> None:\n",
    "        for state in states:\n",
    "                self.G[state] = np.random.uniform(low=-1.0, high=-0.1)\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "- maze definition\n",
    "- update maze\n",
    "- allowed moves\n",
    "- check or game over\n",
    "- get reward\n",
    "- print maze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that is not the agent. \n",
    "- reward\n",
    "- observation\n",
    "- states \n",
    "<br>\n",
    "Robot can not move a wall (robot )\n",
    "check if game is over\n",
    "state of system\n",
    "get reward\n",
    "print maze\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My implementation\n",
    "#### We re going to design a maze:\n",
    "- 6x6\n",
    "- 0,0 entry\n",
    "- 5,5 exit\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### maze\n",
    "* numpy array:\n",
    "  - 0 empty\n",
    "  - 1 wall\n",
    "  - 2 robot"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "maze = np.zeros([6,6], int)\n",
    "\n",
    "def viz_maze(array)-> None:\n",
    "    print (pd.DataFrame(array))\n",
    "\n",
    "def reset_env(env):\n",
    "    env = np.zeros([6,6], int)\n",
    "    env[0,0] = 2 #robot\n",
    "    return env\n",
    "    \n",
    "\n",
    "maze = reset_env(maze)\n",
    "viz_maze(maze)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add Walls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maze = reset_env(maze)\n",
    "\n",
    "def add_walls(env):\n",
    "    for i in range (4):\n",
    "        env [i,5] = 1\n",
    "    for i in range (5):\n",
    "        env [5,i] = 1\n",
    "    for i in range(2,5):\n",
    "        env[2,i] = 1\n",
    "    env[3,2] = 1\n",
    "    return maze\n",
    "\n",
    "        \n",
    "maze = add_walls(maze)\n",
    "viz_maze(maze)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allow move"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow move to zero slots <br>\n",
    "if wall disable move \n",
    "\n",
    "- https://medium.com/@cy.chiang/python-if-else-statement-vs-dictionary-c156b226c04d\n",
    "- https://github.com/cychiang/python-for-fun/blob/master/dict_ifelse/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def location (env) -> list:\n",
    "    agt = np.nonzero(env == 2)\n",
    "    wall = np.nonzero(env == 1)\n",
    "    return agt, wall\n",
    "\n",
    "location(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timesExec = 0\n",
    "\n",
    "def move(dir)->bool:\n",
    "    global timesExec \n",
    "    timesExec += 1\n",
    "    direction ={\n",
    "        'up' : [1,-1,0],\n",
    "        'right': [2, 0, 1],\n",
    "        'down': [3 , 1 ,0],\n",
    "        'left' : [4 , 0 ,-1]\n",
    "    }\n",
    "\n",
    "    if type(dir) != int:\n",
    "        try:\n",
    "            dir = direction[dir][0] #get int of move\n",
    "        except KeyError:\n",
    "            logger.critical(\"ops: {} not support\".format(dir))\n",
    "    else:\n",
    "        if dir > 4:\n",
    "            raise ValueError(\"out of movement range\")\n",
    "        dir = dir\n",
    "    \n",
    "    xdir = 0\n",
    "    ydir = 0\n",
    "    result = dir\n",
    "\n",
    "    for new_k in direction.items():\n",
    "        if new_k[1][0] == dir:\n",
    "            result = new_k[0]\n",
    "            xdir = new_k[1][1]\n",
    "            ydir = new_k[1][2]\n",
    "\n",
    "\n",
    "    print(result, xdir, ydir)\n",
    "\n",
    "    \n",
    "    agt, wall = location(maze)\n",
    "\n",
    "    nagt = [None, None]\n",
    "\n",
    "    nagt[0] = agt[0]+xdir if  0 <= agt[0]+xdir <= 5 else agt[0] # boundary check\n",
    "    nagt[1] = agt[1]+ydir if 0 <= agt[1]+ydir <= 5 else agt[1]\n",
    "\n",
    "    wall = np.transpose(wall)\n",
    "\n",
    "    for x,y in wall:\n",
    "        if nagt[0] == x and nagt[1] == y:\n",
    "            nagt = agt\n",
    "            print ('hit wall')\n",
    "\n",
    "    if agt[0] == nagt[0] and agt[1] == nagt [1]:\n",
    "        print ('no movement')\n",
    "        return 0\n",
    "    else:\n",
    "        maze[agt[0], agt[1]] = 0\n",
    "        maze[nagt[0], nagt[1]] = 2\n",
    "        return 1\n",
    "\n",
    "move (4)\n",
    "viz_maze(maze)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "points = 0 \n",
    "gamma = 0.4\n",
    "def reward(reward, moves, gamma):\n",
    "    reward = reward+gamma**(1/moves)\n",
    "    print (reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if move('left'):    \n",
    "    reward (points, timesExec, gamma)\n",
    "viz_maze(maze)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### School"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maze definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Maze (object):\n",
    "    def __init__(self) -> None:\n",
    "        self.reset_env()\n",
    "\n",
    "\n",
    "    def reset_env(self)->None:\n",
    "        self.maze = np.zeros([6,6], int)\n",
    "        self.maze [:4,5] = 1\n",
    "        self.maze [5,:5] = 1\n",
    "        self.maze[2,2:] = 1\n",
    "        self.maze[3,2] = 1\n",
    "        self.maze[0,0] = 2 #robot\n",
    "        self.robotPostition = (0,0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def printMaze(self)-> None:\n",
    "    print (pd.DataFrame(self.maze))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Allowed Moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def isAloowedMove (self, state, action)-> None:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### update maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def updateMaze (self)-> None:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check for game over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def isGameOver (self)-> None:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def getState (self)-> None:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get reward\n",
    "- dont over specify \n",
    "- randomly assign reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Maze\n",
    "\n",
    "def giveReward (self)-> None:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5\n",
      "0  2  0  0  0  0  1\n",
      "1  0  0  0  0  0  1\n",
      "2  0  0  1  1  1  1\n",
      "3  0  0  1  0  0  1\n",
      "4  0  0  0  0  0  0\n",
      "5  1  1  1  1  1  0\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "env.printMaze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a350d8d21ffb90d17bcf02bc3212a3330f6be005e495f54e618b0bdcf6a5456b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
